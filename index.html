<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Face Analyzer</title>
  <script defer src="https://cdn.jsdelivr.net/npm/face-api.js"></script>
  <style>
    body {
      display: flex;
      flex-direction: column;
      align-items: center;
      background: #0f172a;
      color: #fff;
      font-family: Arial, sans-serif;
      height: 100vh;
      justify-content: center;
      text-align: center;
    }
    video, canvas, img {
      border-radius: 12px;
      margin-top: 12px;
      max-width: 90%;
    }
    button {
      margin: 8px;
      padding: 10px 20px;
      font-size: 1rem;
      border: none;
      border-radius: 6px;
      background-color: #2563eb;
      color: white;
      cursor: pointer;
    }
    button:hover {
      background-color: #1d4ed8;
    }
  </style>
</head>
<body>
  <h1>Face Analyzer</h1>
  <p id="status">Loading models...</p>

  <div>
    <button id="startCamera">ðŸ“· Use Live Camera</button>
    <input type="file" id="upload" accept="image/*">
  </div>

  <video id="video" width="640" height="480" autoplay muted></video>
  <canvas id="overlay" width="640" height="480"></canvas>
  <img id="photo" hidden />

  <script>
    const video = document.getElementById('video');
    const overlay = document.getElementById('overlay');
    const upload = document.getElementById('upload');
    const photo = document.getElementById('photo');
    const status = document.getElementById('status');
    const startCameraBtn = document.getElementById('startCamera');

    async function loadModels() {
      const MODEL_URL = './models'; // ðŸ‘ˆ relative path
      status.textContent = 'Loading models...';

      await Promise.all([
        faceapi.nets.ssdMobilenetv1.loadFromUri(MODEL_URL),
        faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),
        faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL)
      ]);

      status.textContent = 'Models loaded. Choose camera or upload a photo.';
    }

    async function startCamera() {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ video: true });
        video.srcObject = stream;
        video.onplay = () => analyzeVideo();
        status.textContent = 'Camera started. Analyzing...';
      } catch (err) {
        console.error('Camera error:', err);
        status.textContent = 'Camera access denied or not available.';
      }
    }

    async function analyzeVideo() {
      const displaySize = { width: video.width, height: video.height };
      faceapi.matchDimensions(overlay, displaySize);

      setInterval(async () => {
        const detections = await faceapi
          .detectAllFaces(video)
          .withFaceLandmarks()
          .withFaceExpressions();

        const resized = faceapi.resizeResults(detections, displaySize);
        overlay.getContext('2d').clearRect(0, 0, overlay.width, overlay.height);
        faceapi.draw.drawDetections(overlay, resized);
        faceapi.draw.drawFaceLandmarks(overlay, resized);
        faceapi.draw.drawFaceExpressions(overlay, resized);
      }, 300);
    }

    upload.addEventListener('change', async () => {
      const file = upload.files[0];
      if (!file) return;

      status.textContent = 'Analyzing photo...';
      const img = await faceapi.bufferToImage(file);
      photo.src = img.src;
      photo.hidden = false;
      video.hidden = true;
      overlay.hidden = true;

      const detections = await faceapi
        .detectAllFaces(photo)
        .withFaceLandmarks()
        .withFaceExpressions();

      if (detections.length === 0) {
        status.textContent = 'No face detected.';
        return;
      }

      const canvas = faceapi.createCanvasFromMedia(photo);
      document.body.append(canvas);
      const displaySize = { width: photo.width, height: photo.height };
      faceapi.matchDimensions(canvas, displaySize);
      const resized = faceapi.resizeResults(detections, displaySize);
      faceapi.draw.drawDetections(canvas, resized);
      faceapi.draw.drawFaceLandmarks(canvas, resized);
      faceapi.draw.drawFaceExpressions(canvas, resized);
      status.textContent = 'Photo analysis complete!';
    });

    startCameraBtn.addEventListener('click', startCamera);

    loadModels();
  </script>
</body>
</html>
